{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Module for Gaussian process regression\n",
    "\n",
    "\"\"\"\n",
    "import gpytorch\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Spatiotemporal_GP(gpytorch.models.ExactGP):\n",
    "    \"\"\"Spatiotemporal Gaussian Process model class\n",
    "\n",
    "    input:\n",
    "\n",
    "        mean_function: mean function - from gpytorch classes or torch.nn\n",
    "        train_x: training features: Nxp dimensions\n",
    "        train_y: training labels: Nx1 dimensions\n",
    "        likelihood: Specify the likelihood function - from gpytorch classes\n",
    "    \"\"\"\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(Spatiotemporal_GP, self).__init__(train_x, train_y, likelihood)\n",
    "\n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_month = gpytorch.kernels.MaternKernel()\n",
    "        self.covar_year = gpytorch.kernels.MaternKernel()\n",
    "        self.covar_spatial = gpytorch.kernels.MaternKernel()\n",
    "        self.covar_prop = gpytorch.kernels.MaternKernel()\n",
    "\n",
    "        #self.mean_module = torch.nn.Linear(train_x.shape[1], 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass of GP model\n",
    "\n",
    "        \"\"\"\n",
    "        year = x.narrow(1,0,1)\n",
    "        month = x.narrow(1,1,1)\n",
    "        spatial = x.narrow(1,3,2)\n",
    "        prop = x.narrow(1, 2, 1)\n",
    "        # prevent period to reset\n",
    "        mean = self.mean_module(x).view(-1)\n",
    "\n",
    "        #compute covariances\n",
    "        #self.covar_season.period_length = Tensor([1]) # year seasonality indicating 1\n",
    "        covar_prop = self.covar_prop(year)\n",
    "        covar_month = self.covar_month(month)\n",
    "        covar_spatial = self.covar_spatial(spatial)\n",
    "        covar_prop = self.covar_prop(prop)\n",
    "        \n",
    "        covariance = covar_prop*covar_month*covar_spatial*covar_month\n",
    "\n",
    "        return gpytorch.distributions.MultivariateNormal(mean, covariance)\n",
    "\n",
    "def train_model(train_x, train_y, model, likelihood, epochs = 50):\n",
    "    \"\"\"training procedure\n",
    "\n",
    "    input:\n",
    "\n",
    "        model: model object\n",
    "        likelihood: likelihood object\n",
    "        epochs: number of training epochs\n",
    "\n",
    "    output:\n",
    "\n",
    "        model: trained posterior model\n",
    "\n",
    "    \"\"\"\n",
    "    # Find optimal model hyperparameters\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam([\n",
    "        {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "    ], lr=0.1)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        print(\"Iter {}/{} - Loss: {}\".format(\n",
    "            i + 1, epochs, loss.item()\n",
    "        ))\n",
    "\n",
    "        # if you are using lengthscale kernels\n",
    "        #print(\"Iter {}/{} - Loss: {} lengthscale_s {}, lengthscale_t {}\".format(\n",
    "        #    i + 1, epochs, loss.item(),\n",
    "        #    model.covar_module_s.lengthscale.item(),\n",
    "        #    model.covar_module_t.lengthscale.item()\n",
    "        #))\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict(test_x, model, likelihood):\n",
    "    \"\"\"compute posterior predictive mean and variance\n",
    "\n",
    "    input:\n",
    "\n",
    "        test_x: covariates of test in matrix form\n",
    "        model: GP model object\n",
    "        likelihood: likelihood object\n",
    "\n",
    "    output:\n",
    "\n",
    "        posterior_pred: return posterior prediction objected\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions by feeding model through likelihood\n",
    "    # gpytorch.settings.fast_pred_var() for LOVE prediction\n",
    "    with torch.no_grad():\n",
    "        posterior_pred = likelihood(model(test_x))\n",
    "\n",
    "    return posterior_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training \n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-37289ea0276d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Training \\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgp_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgp_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# posterior prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-c85761996348>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_x, train_y, model, likelihood, epochs)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;31m# Calc loss and backprop gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mmll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         print(\"Iter {}/{} - Loss: {}\".format(\n\u001b[1;32m     82\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/citadel-challenge-2.0/.env/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/citadel-challenge-2.0/.env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0mgrad_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work/citadel-challenge-2.0/.env/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../clean_data/agg_join.csv')\n",
    "df = df[df['year']>2018]\n",
    "\n",
    "X = df.loc[:, ['year', 'month', 'Pct_Remain', 'lat', 'lon']].values\n",
    "y = df.loc[:, ['num_jobs']].values.reshape((X.shape[0], 1))\n",
    "\n",
    "train_x = Tensor(X)\n",
    "train_y = Tensor(y)\n",
    "\n",
    "train_x = train_x.cuda()\n",
    "train_y = train_y.cuda()\n",
    "#     test_x = test_x.cuda()\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood().cuda()\n",
    "gp_model = Spatiotemporal_GP(train_x, train_y, likelihood).cuda()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    likelihood = likelihood.cuda()\n",
    "    gp_model = gp_model.cuda()\n",
    "\n",
    "print(\"Start Training \\n\")\n",
    "gp_model = train_model(train_x, train_y, gp_model, likelihood, epochs=200)\n",
    "\n",
    "# posterior prediction\n",
    "#     posterior_pred = predict(test_x, gp_model, likelihood)\n",
    "    \n",
    "#     if torch.cuda.is_available():\n",
    "#         posterior_mean = posterior_pred.mean.cpu()\n",
    "#         print(\"Test RSME:{}\".format(torch.norm(test_y - posterior_mean)/ posterior_mean.shape[0]))\n",
    "\n",
    "#     else:\n",
    "#         posterior_mean = posterior_pred.mean\n",
    "#         print(\"Test RSME:{}\".format((torch.norm(test_y - posterior_mean) / posterior_mean.shape[0])))\n",
    "    \n",
    "#     #plt.figure(0)\n",
    "#     #plt.scatter([i for i in range(1,223)], test_y.cpu(), marker = \".\")\n",
    "#     #plt.scatter([i for i in range(1,223)], posterior_pred.mean.cpu().numpy())\n",
    "#     #plt.show()\n",
    "\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#     # Initialize plot\n",
    "#         f, ax = plt.subplots(1, 1, figsize=(12, 12))\n",
    "#         plt.plot(test_y.cpu().numpy(), test_y.cpu().numpy())\n",
    "#         ax.scatter(train_max*(posterior_pred.mean.cpu().numpy() + 1)/2, test_y.cpu().numpy(), marker=\".\")\n",
    "#         #plt.ylim((0, 2000))\n",
    "#         #plt.xlim((0, 2000))\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cit",
   "language": "python",
   "name": "cit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
